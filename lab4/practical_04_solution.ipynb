{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "\n",
    "W1 = .1 * np.ones((units[1], units[0]))\n",
    "b1 = .1 * np.ones(units[1])\n",
    "W2 = .1 * np.ones((units[2], units[1]))\n",
    "b2 = .1 * np.ones(units[2])\n",
    "W3 = .1 * np.ones((units[3], units[2]))\n",
    "b3 = .1 * np.ones(units[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16396106 0.16396106 0.16396106]\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "\n",
    "h0 = inputs[0]\n",
    "\n",
    "z1 = W1.dot(h0) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = W2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = W3.dot(h2) + b3\n",
    "\n",
    "y = labels[0]\n",
    "\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37636378397755565\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "\n",
    "loss = .5*(z3 - y).dot(z3 - y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "\n",
    "grad_z3 = z3 - y  # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W3 = grad_z3[:, None].dot(h2[:, None].T)\n",
    "grad_b3 = grad_z3\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h2 = W3.T.dot(grad_z3)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z2 = grad_h2 * (1-h2**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W2 = grad_z2[:, None].dot(h1[:, None].T)\n",
    "grad_b2 = grad_z2\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h1 = W2.T.dot(grad_z2)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z1 = grad_h1 * (1-h1**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W1 = grad_z1[:, None].dot(h0[:, None].T)\n",
    "grad_b1 = grad_z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "\n",
    "# Gradient updates.\n",
    "eta = 0.1\n",
    "W1 -= eta*grad_W1\n",
    "b1 -= eta*grad_b1\n",
    "W2 -= eta*grad_W2\n",
    "b2 -= eta*grad_b2\n",
    "W3 -= eta*grad_W3\n",
    "b3 -= eta*grad_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "W1 = .1 * np.ones((units[1], units[0]))\n",
    "b1 = .1 * np.ones(units[1])\n",
    "W2 = .1 * np.ones((units[2], units[1]))\n",
    "b2 = .1 * np.ones(units[2])\n",
    "W3 = .1 * np.ones((units[3], units[2]))\n",
    "b3 = .1 * np.ones(units[3])\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "h0 = inputs[0]\n",
    "\n",
    "z1 = W1.dot(h0) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = W2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = W3.dot(h2) + b3\n",
    "\n",
    "p = np.exp(z3) / sum(np.exp(z3))\n",
    "y = labels[0]\n",
    "\n",
    "# Loss\n",
    "\n",
    "loss = -y.dot(np.log(p))\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "grad_z3 = p - y  # Grad of loss wrt p\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W3 = grad_z3[:, None].dot(h2[:, None].T)\n",
    "grad_b3 = grad_z3\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h2 = W3.T.dot(grad_z3)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z2 = grad_h2 * (1-h2**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W2 = grad_z2[:, None].dot(h1[:, None].T)\n",
    "grad_b2 = grad_z2\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h1 = W2.T.dot(grad_z2)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z1 = grad_h1 * (1-h1**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W1 = grad_z1[:, None].dot(h0[:, None].T)\n",
    "grad_b1 = grad_z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    hiddens = []\n",
    "    # compute hidden layers\n",
    "    for i in range(num_layers):\n",
    "            h = x if i == 0 else hiddens[i-1]\n",
    "            z = weights[i].dot(h) + biases[i]\n",
    "            if i < num_layers-1:  # Assuming the output layer has no activation.\n",
    "                hiddens.append(g(z))\n",
    "    #compute output\n",
    "    output = z\n",
    "    \n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    # compute loss\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    loss = -y.dot(np.log(probs))\n",
    "    \n",
    "    return loss   \n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    z = output\n",
    "\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    grad_z = probs - y  \n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations \n",
    "    for i in range(num_layers-1, -1, -1):\n",
    "        \n",
    "        # Gradient of hidden parameters.\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        grad_weights.append(grad_z[:, None].dot(h[:, None].T))\n",
    "        grad_biases.append(grad_z)\n",
    "        \n",
    "        # Gradient of hidden layer below.\n",
    "        grad_h = weights[i].T.dot(grad_z)\n",
    "\n",
    "        # Gradient of hidden layer below before activation.\n",
    "        grad_z = grad_h * (1-h**2)   # Grad of loss wrt z3.\n",
    "\n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0\n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Comoute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        \n",
    "        # Compute Loss and Update total loss\n",
    "        loss = compute_loss(output, y)\n",
    "        total_loss+=loss\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)\n",
    "        \n",
    "        # Update weights\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            weights[i] -= eta*grad_weights[i]\n",
    "            biases[i] -= eta*grad_biases[i]\n",
    "            \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "\n",
    "# Single hidden unit MLP with 50 hidden units.\n",
    "# First is input size, last is output size.\n",
    "units = [64, 50, 10]\n",
    "\n",
    "# Initialize all weights and biases randomly.\n",
    "W1 = .1 * np.random.randn(units[1], units[0])\n",
    "b1 = .1 * np.random.randn(units[1])\n",
    "W2 = .1 * np.random.randn(units[2], units[1])\n",
    "b2 = .1 * np.random.randn(units[2])\n",
    "\n",
    "weights = [W1, W2]\n",
    "biases = [b1, b2]\n",
    "\n",
    "# Learning rate.\n",
    "eta = 0.001  \n",
    "    \n",
    "# Run epochs\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    weights, biases, loss = MLP_train_epoch(X_train, y_train_ohe, weights, biases)\n",
    "    losses.append(loss)\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        # Compute forward pass and get the class with the highest probability\n",
    "        output, _ = forward(x, weights, biases)\n",
    "        y_hat = np.argmax(output)\n",
    "        predicted_labels.append(y_hat)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9993041057759221\n",
      "Test accuracy: 0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = MLP_predict(X_train, weights, biases)\n",
    "y_test_pred = MLP_predict(X_test, weights, biases)\n",
    "\n",
    "print(f'Train accuracy: {(y_train_pred==y_train).mean()}')\n",
    "print(f'Test accuracy: {(y_test_pred==y_test).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993041057759221\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
