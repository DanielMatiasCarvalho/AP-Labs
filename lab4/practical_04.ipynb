{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w1 = np.ones((4,4)) * 0.1\n",
    "w2 = np.ones((3,4)) * 0.1\n",
    "w3 = np.ones((3,3)) * 0.1\n",
    "b1 = np.ones((4)) * 0.1\n",
    "b2 = np.ones((3)) * 0.1\n",
    "b3 = np.ones((3)) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "x = inputs[0]\n",
    "y = labels[0]\n",
    "\n",
    "z1 = w1.dot(x) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = w2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = w3.dot(h2) + b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37636378397755565"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss = 0.5 * (z3 - y).dot(z3 - y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "z3_grad = z3 - y\n",
    "\n",
    "# [:, None] adds a dimension\n",
    "w3_grad = z3_grad[:, None].dot(h2[:, None].T)\n",
    "b3_grad = z3_grad\n",
    "h2_grad = w3.T.dot(z3_grad)\n",
    "z2_grad = h2_grad * (1-np.tanh(z2)**2)\n",
    "\n",
    "w2_grad = z2_grad[:, None].dot(h1[:, None].T)\n",
    "b2_grad = z2_grad\n",
    "h1_grad = w2.T.dot(z2_grad)\n",
    "z1_grad = h1_grad * (1-np.tanh(z1)**2)\n",
    "\n",
    "w1_grad = z1_grad[:, None].dot(x[:, None].T)\n",
    "b1_grad = z1_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "eta = 0.1\n",
    "\n",
    "\n",
    "w1 = w1 - eta * w1_grad\n",
    "w2 = w2 - eta * w2_grad\n",
    "w3 = w3 - eta * w3_grad\n",
    "\n",
    "b1 = b1 - eta * b1_grad\n",
    "b2 = b2 - eta * b2_grad\n",
    "b3 = b3 - eta * b3_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent:\n",
    "Foward propagation for all data points\n",
    "\n",
    "Backpropagation with the original parameters for each data points\n",
    "\n",
    "Sum the gradients of all data points and update weights\n",
    "\n",
    "Learning rate absorbs the 1/N of the gradient descent, so we could put it or not, depends\n",
    "\n",
    "Stochastic Gradient Descent:\n",
    "Foward propagation of a data point\n",
    "\n",
    "Backpropagation with it\n",
    "\n",
    "Update the parameters with its gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0, 0, 10, 0]])\n",
    "labels = np.array([[0, 0, 1]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w1 = np.ones((4,4)) * 0.1\n",
    "w2 = np.ones((3,4)) * 0.1\n",
    "w3 = np.ones((3,3)) * 0.1\n",
    "b1 = np.ones((4)) * 0.1\n",
    "b2 = np.ones((3)) * 0.1\n",
    "b3 = np.ones((3)) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "x = inputs[0]\n",
    "y = labels[0]\n",
    "\n",
    "z1 = w1.dot(x) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = w2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = w3.dot(h2) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729260865456343"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss1 = 0.5 * (z3 - y).dot(z3 - y)\n",
    "loss + loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "z3_grad1 = z3 - y\n",
    "\n",
    "# [:, None] adds a dimension\n",
    "w3_grad1 = z3_grad1[:, None].dot(h2[:, None].T)\n",
    "b3_grad1 = z3_grad1\n",
    "h2_grad1 = w3.T.dot(z3_grad1)\n",
    "z2_grad1 = h2_grad1 * (1-np.tanh(z2)**2)\n",
    "\n",
    "w2_grad1 = z2_grad1[:, None].dot(h1[:, None].T)\n",
    "b2_grad1 = z2_grad1\n",
    "h1_grad1 = w2.T.dot(z2_grad1)\n",
    "z1_grad1 = h1_grad1 * (1-np.tanh(z1)**2)\n",
    "\n",
    "w1_grad1 = z1_grad1[:, None].dot(x[:, None].T)\n",
    "b1_grad1 = z1_grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "eta = 0.1\n",
    "\n",
    "w1 = w1 - eta * (w1_grad + w1_grad1)\n",
    "w2 = w2 - eta * (w2_grad + w2_grad1)\n",
    "w3 = w3 - eta * (w3_grad + w3_grad1)\n",
    "\n",
    "b1 = b1 - eta * (b1_grad + b1_grad1)\n",
    "b2 = b2 - eta * (b2_grad + b2_grad1)\n",
    "b3 = b3 - eta * (b3_grad + b3_grad1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w1 = np.ones((4,4)) * 0.1\n",
    "w2 = np.ones((3,4)) * 0.1\n",
    "w3 = np.ones((3,3)) * 0.1\n",
    "b1 = np.ones((4)) * 0.1\n",
    "b2 = np.ones((3)) * 0.1\n",
    "b3 = np.ones((3)) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "x = inputs[0]\n",
    "y = labels[0]\n",
    "\n",
    "z1 = w1.dot(x) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = w2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = w3.dot(h2) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "exp_scores = np.exp(z3)\n",
    "z_value = np.sum(exp_scores)\n",
    "y_prob = exp_scores / z_value\n",
    "\n",
    "correct_class = np.argmax(y)\n",
    "\n",
    "loss = np.log(z_value) - z3[correct_class]\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "z3_grad = y_prob - y\n",
    "\n",
    "# [:, None] adds a dimension\n",
    "w3_grad = z3_grad[:, None].dot(h2[:, None].T)\n",
    "b3_grad = z3_grad\n",
    "h2_grad = w3.T.dot(z3_grad)\n",
    "z2_grad = h2_grad * (1-np.tanh(z2)**2)\n",
    "\n",
    "w2_grad = z2_grad[:, None].dot(h1[:, None].T)\n",
    "b2_grad = z2_grad\n",
    "h1_grad = w2.T.dot(z2_grad)\n",
    "z1_grad = h1_grad * (1-np.tanh(z1)**2)\n",
    "\n",
    "w1_grad = z1_grad[:, None].dot(x[:, None].T)\n",
    "b1_grad = z1_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "eta = 0.1\n",
    "\n",
    "w1 = w1 - eta * w1_grad\n",
    "w2 = w2 - eta * w2_grad\n",
    "w3 = w3 - eta * w3_grad\n",
    "\n",
    "b1 = b1 - eta * b1_grad\n",
    "b2 = b2 - eta * b2_grad\n",
    "b3 = b3 - eta * b3_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0, 0, 10, 0]])\n",
    "labels = np.array([[0, 0, 1]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w1 = np.ones((4,4)) * 0.1\n",
    "w2 = np.ones((3,4)) * 0.1\n",
    "w3 = np.ones((3,3)) * 0.1\n",
    "b1 = np.ones((4)) * 0.1\n",
    "b2 = np.ones((3)) * 0.1\n",
    "b3 = np.ones((3)) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "x = inputs[0]\n",
    "y = labels[0]\n",
    "\n",
    "z1 = w1.dot(x) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = w2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = w3.dot(h2) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1972245773362196"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "exp_scores = np.exp(z3)\n",
    "z_value = np.sum(exp_scores)\n",
    "y_prob = exp_scores / z_value\n",
    "\n",
    "correct_class = np.argmax(y)\n",
    "\n",
    "loss1 = np.log(z_value) - z3[correct_class]\n",
    "loss + loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "z3_grad1 = y_prob - y\n",
    "\n",
    "# [:, None] adds a dimension\n",
    "w3_grad1 = z3_grad1[:, None].dot(h2[:, None].T)\n",
    "b3_grad1 = z3_grad1\n",
    "h2_grad1 = w3.T.dot(z3_grad1)\n",
    "z2_grad1 = h2_grad1 * (1-np.tanh(z2)**2)\n",
    "\n",
    "w2_grad1 = z2_grad1[:, None].dot(h1[:, None].T)\n",
    "b2_grad1 = z2_grad1\n",
    "h1_grad1 = w2.T.dot(z2_grad1)\n",
    "z1_grad1 = h1_grad1 * (1-np.tanh(z1)**2)\n",
    "\n",
    "w1_grad1 = z1_grad1[:, None].dot(x[:, None].T)\n",
    "b1_grad1 = z1_grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "eta = 0.1\n",
    "\n",
    "w1 = w1 - eta * w1_grad\n",
    "w2 = w2 - eta * w2_grad\n",
    "w3 = w3 - eta * w3_grad\n",
    "\n",
    "b1 = b1 - eta * b1_grad\n",
    "b2 = b2 - eta * b2_grad\n",
    "b3 = b3 - eta * b3_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    hiddens = []\n",
    "    \n",
    "    z1 = weights[0].dot(x) + biases[0]\n",
    "    h1 = g(z1)\n",
    "    hiddens.append((z1, h1))\n",
    "    \n",
    "    # compute hidden layers\n",
    "    for i in range(1, num_layers-1):\n",
    "        z = weights[i].dot(hiddens[i-1][1]) + biases[i]\n",
    "        h = g(z)\n",
    "        hiddens.append((z, h))\n",
    "    \n",
    "    #compute output\n",
    "    if num_layers == 1:\n",
    "        output = z1\n",
    "    else:\n",
    "        output = weights[-1].dot(hiddens[-1][1]) + biases[-1]\n",
    "    \n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    # Loss\n",
    "    exp_scores = np.exp(output)\n",
    "    z_value = np.sum(exp_scores)\n",
    "\n",
    "    correct_class = np.argmax(y)\n",
    "\n",
    "    loss = np.log(z_value) - output[correct_class]\n",
    "    \n",
    "    return loss   \n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    z = output\n",
    "    \n",
    "    probs = np.exp(z) / np.sum(np.exp(z))\n",
    "    grad_z = probs - y\n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations \n",
    "    for i in range(num_layers-1, 0, -1):\n",
    "        # Gradient of hidden parameters.\n",
    "        grad_weights.append(grad_z[:, None].dot(hiddens[i-1][1][:, None].T))\n",
    "        grad_biases.append(grad_z)\n",
    "        grad_h = weights[i].T.dot(grad_z)\n",
    "        grad_z = grad_h * (1 - g(hiddens[i-1][0])**2)\n",
    "    \n",
    "    grad_weights.append(grad_z[:, None].dot(x[:, None].T))\n",
    "    grad_biases.append(grad_z)\n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases\n",
    "\n",
    "def update_weights(weights, grad_weights, biases, grad_biases, eta = 0.01):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= eta * grad_weights[i]\n",
    "        biases[i] -= eta * grad_biases[i]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases, eta = 0.01):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        # Compute Loss and update total loss\n",
    "        total_loss += compute_loss(output, y)\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)\n",
    "        # Update weights\n",
    "        weights, biases = update_weights(weights, grad_weights, biases, grad_biases, eta)\n",
    "        \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE TO SELF: NEVER, EVER, try to initialize weights and biases all with the same value. Symmetry breaking is important.\n",
    "\n",
    "# Initialize weights\n",
    "w1 = np.random.randn(50, p) * 0.01\n",
    "w2 = np.random.randn(n_classes, 50) * 0.01\n",
    "w = [w1, w2]\n",
    "\n",
    "b1 = np.random.randn(50) * 0.01\n",
    "b2 = np.random.randn(n_classes) * 0.01\n",
    "b = [b1, b2]\n",
    "\n",
    "# Empty loss list\n",
    "loss_list = []\n",
    "\n",
    "# Learning rate.\n",
    "eta = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Run epochs and append loss to list\n",
    "for i in range(epochs):\n",
    "    w, b, loss = MLP_train_epoch(X_train, y_train_ohe, w, b, eta)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "# Plot loss evolution\n",
    "\n",
    "# Plot the loss evolution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 101), loss_list, marker='o', linestyle='-', color='b')\n",
    "plt.title('Loss Evolution Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        output, _ = forward(x, weights, biases)\n",
    "        predicted_labels.append(np.argmax(output))\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_train = MLP_predict(X_train, w, b)\n",
    "predicted_labels_test = MLP_predict(X_test, w, b)\n",
    "\n",
    "print(f'Train accuracy: {(predicted_labels_train==y_train).mean()}')\n",
    "print(f'Test accuracy: {(predicted_labels_test==y_test).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993041057759221\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
